---
published: false
layout: single
title: "핸즈온 머신러닝 정리 - 4장"
category: Book
comments: true
toc: true
toc_sticky: true
use_math: true
---

# 모델 훈련

이 장에서는 가장 간단한 모델 중 하나인 선형 회귀부터 시작합니다. 이 모델을 훈련시키는 두 가지 방법을 설명하겠습니다.

- 직접 계산할 수 있는 공식을 사용하여 훈련 세트에 가장 잘 맞는 모델 파라미터를 해석적으로 구합니다.
- 경사 하강법이라 불리는 반복적인 최적화 방식을 사용하여 모델 파라미터를 조금씩 바꾸면서 비용 함수를 훈련 세트에 대해 최소화시킵니다. 결국에는 앞의 방법과 동일한 파라미터로 수렴합니다. 경사 하강법의 변종으로 2부에서 신경망을 공부할 때 계속 사용하게 될 배치 경사 하강법, 미니배치 경사 하강법, 확률적 경사 하강법도 살펴보겠습니다.

그다음에는 비선형 데이터셋에 훈련시킬 수 있는 조금 더 복잡한 모델인 다항 회귀를 살펴보겠습니다. 마지막으로 분류 작업에 널리 사용하는 모델인 로지스틱 회귀와 소프트맥스 회귀를 살펴보겠습니다.

<br/>

**선형 회귀**

선형 회귀 모델을 훈련시키려면 RMSE를 최소화하는 *θ*를 찾아야 합니다. 실제로는 RMSE보다 평균 제곱 오차(MSE)를 최소화하는 것이 같은 결과를 내면서 더 간단합니다.
$$
MSE(X,h_\theta)={\frac 1m}\sum\limits_{i=1}^n(\theta^{T}\cdot x^{(i)}-y^{(i)})
$$
훈련세트 X에 대한 선형 회귀 가설 h<sub>*θ*</sub>의 MSE는 위 식처럼 계산합니다.

<br/>

**정규방정식**

비용 함수를 최소화하는 *θ* 값을 찾기 위한 해석적인 방법이 있습니다. 다른 말로 하면 바로 결과를 얻을 수 있는 수학 공식이 있습니다. 이를 정규방정식이라고 합니다.
$$
\hat\theta=(X^T\cdot {X})^{-1}\cdot {X^T}\cdot {y}
$$
<br/>

**계산 복잡도**

정규방정식은 (n+1) x (n+1) 크기가 되는 X<sup>T</sup>X의 역행렬을 계산합니다(n은 특성 수). 역행렬을 계산하는 계산 복잡도는 일반적으로 O(n<sup>2.4</sup>)에서 O(n<sup>3</sup>) 사이입니다. 다시 말해 특성 수가 두 배로 늘어나면 계산 시간이 5.3배에서 8배로 증가합니다. 다행인 것은 이 공식의 복잡도가 훈련 세트의 샘플 수에는 선형적으로 증가합니다. 그러므로 메모리 공간이 허락된다면 큰 훈련 세트도 효율적으로 처리할 수 있습니다.

<br/>

**경사 하강법**

경사하강법은 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 매우 일반적인 최적화 알고리즘입니다. 경사 하강법의 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조정해가는 것입니다.

구체적으로 보면 *θ*를 임의의 값으로 시작해서 한 번에 조금씩 비용 함수가 감소되는 방향으로 진행하여 알고리즘이 최솟값에 수렴할 때까지 점진적으로 향상시킵니다.

경사 하강법에서 중요한 파라미터는 스텝의 크기로, 학습률 하이퍼파라미터로 결정됩니다.

다행히 선형 회귀를 위한 MSE 비용 함수는 곡선에서 어떤 두 점을 선택해 선을 그어도 곡선을 가로지르지 않는 볼록 함수입니다. 이는 지역 최솟값이 없고 하나의 전역 최솟값만 있다는 뜻입니다. 또한 연속된 함수이고 기울기가 갑자기 변하지 않습니다.

<br/>

**배치 경사 하강법**

경사 하강법을 구현하려면 각 모델 파라미터 *θ*에 대해 비용 함수의 그래디언트를 계산해야 합니다. 다시 말해 *θ*가 조금 변경될 때 비용 함수가 얼마나 바뀌는지 계산해야 합니다. 이를 편도함수라고 합니다.