---
published: true
layout: single
title: "핸즈온 머신러닝 정리 - 4장"
category: Book
comments: true
toc: true
toc_sticky: true
use_math: true
---

# 모델 훈련

이 장에서는 가장 간단한 모델 중 하나인 선형 회귀부터 시작합니다. 이 모델을 훈련시키는 두 가지 방법을 설명하겠습니다.

- 직접 계산할 수 있는 공식을 사용하여 훈련 세트에 가장 잘 맞는 모델 파라미터를 해석적으로 구합니다.
- 경사 하강법이라 불리는 반복적인 최적화 방식을 사용하여 모델 파라미터를 조금씩 바꾸면서 비용 함수를 훈련 세트에 대해 최소화시킵니다. 결국에는 앞의 방법과 동일한 파라미터로 수렴합니다. 경사 하강법의 변종으로 2부에서 신경망을 공부할 때 계속 사용하게 될 배치 경사 하강법, 미니배치 경사 하강법, 확률적 경사 하강법도 살펴보겠습니다.

그다음에는 비선형 데이터셋에 훈련시킬 수 있는 조금 더 복잡한 모델인 다항 회귀를 살펴보겠습니다. 마지막으로 분류 작업에 널리 사용하는 모델인 로지스틱 회귀와 소프트맥스 회귀를 살펴보겠습니다.



## 선형 회귀

선형 회귀 모델을 훈련시키려면 RMSE를 최소화하는 *θ*를 찾아야 합니다. 실제로는 RMSE보다 평균 제곱 오차(MSE)를 최소화하는 것이 같은 결과를 내면서 더 간단합니다.


$$
MSE(X,h_\theta)={\frac 1m}\sum\limits_{i=1}^n(\theta^{T}\cdot x^{(i)}-y^{(i)})
$$


훈련세트 X에 대한 선형 회귀 가설 h<sub>*θ*</sub>의 MSE는 위 식처럼 계산합니다.



### 정규방정식

비용 함수를 최소화하는 *θ* 값을 찾기 위한 해석적인 방법이 있습니다. 다른 말로 하면 바로 결과를 얻을 수 있는 수학 공식이 있습니다. 이를 정규방정식이라고 합니다.


$$
\hat\theta=(X^T\cdot {X})^{-1}\cdot {X^T}\cdot {y}
$$


### 계산 복잡도

정규방정식은 (n+1) x (n+1) 크기가 되는 X<sup>T</sup>X의 역행렬을 계산합니다(n은 특성 수). 역행렬을 계산하는 계산 복잡도는 일반적으로 O(n<sup>2.4</sup>)에서 O(n<sup>3</sup>) 사이입니다. 다시 말해 특성 수가 두 배로 늘어나면 계산 시간이 5.3배에서 8배로 증가합니다. 다행인 것은 이 공식의 복잡도가 훈련 세트의 샘플 수에는 선형적으로 증가합니다. 그러므로 메모리 공간이 허락된다면 큰 훈련 세트도 효율적으로 처리할 수 있습니다.



### 경사 하강법

경사하강법은 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 매우 일반적인 최적화 알고리즘입니다. 경사 하강법의 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조정해가는 것입니다.

구체적으로 보면 *θ*를 임의의 값으로 시작해서 한 번에 조금씩 비용 함수가 감소되는 방향으로 진행하여 알고리즘이 최솟값에 수렴할 때까지 점진적으로 향상시킵니다.

경사 하강법에서 중요한 파라미터는 스텝의 크기로, 학습률 하이퍼파라미터로 결정됩니다.

다행히 선형 회귀를 위한 MSE 비용 함수는 곡선에서 어떤 두 점을 선택해 선을 그어도 곡선을 가로지르지 않는 볼록 함수입니다. 이는 지역 최솟값이 없고 하나의 전역 최솟값만 있다는 뜻입니다. 또한 연속된 함수이고 기울기가 갑자기 변하지 않습니다.



### 배치 경사 하강법

경사 하강법을 구현하려면 각 모델 파라미터 *θ*에 대해 비용 함수의 그래디언트를 계산해야 합니다. 다시 말해 *θ*가 조금 변경될 때 비용 함수가 얼마나 바뀌는지 계산해야 합니다. 이를 편도함수라고 합니다. 그래디언트 벡터는 비용 함수의 편도함수를 모두 담고 있으며 이 알고리즘을 배치 경사 하강법이라고 합니다.

적절한 학습률을 찾으려면 그리드 탐색을 사용합니다. 하지만 그리드 탐색에서 수렴하는 데 너무 오래 걸리는 모델을 막기 위해 반복 횟수를 제한해야 합니다.

반복 횟수를 제한할 때는 반복 횟수를 아주 크게 지정하고 그래디언트 벡터가 아주 작아지면, 즉 벡터의 노름이 어떤 값(허용오차)보다 작아지면 경사 하강법이 (거의) 최솟값에 도달한 것이므로 알고리즘을 중지하는 것입니다.



### 확률적 경사 하강법

배치 경사 하강법의 가장 큰 문제는 매 스텝에서 전체 훈련 세트를 사용해 그래디언트를 계산한다는 사실입니다. 이와는 정반대로 확률적 경사 하강법은 매 스텝에서 딱 한 개의 샘플을 무작위로 선택하고 그 하나의 샘플에 대한 그래디언트를 계산합니다. 매 반복에서 매우 적은 데이터만 처리하기 때문에 알고리즘이 확실히 훨씬 빠릅니다.

반면에, 확률적이기 때문에 이 알고리즘은 배치 경사 하강법보다 훨씬 불안정합니다. 알고리즘이 멈출 때 좋은 파라미터가 구해지겠지만 최적치는 아닙니다.

무작위성은 지역 최솟값에서 탈출시켜줘서 좋지만 알고리즘을 전역 최솟값에 다다르지 못하게 한다는 점에서는 좋지 않습니다. 이 딜레마를 해결하는 한 가지 방법은 학습률을 점진적으로 감소시키는 것입니다. 매 반복마다 학습률을 결정하는 함수를 학습 스케줄이라고 부릅니다.



### 미니배치 경사 하강법

각 스텝에서 전체 훈련 세트나 하나의 샘플을 기반으로 그래디언트를 계산하는 것이 아니라 미니배치라 부르는 임의의 작은 샘플 세트에 대해 그래디언트를 계산합니다. 확률적 경사 하강법에 비해 미니배치 경사 하강법의 주요 장점은 행렬 연산에 최적화된 하드웨어, 특히 GPU를 사용해서 얻는 성능 향상입니다.



### 알고리즘 비교

|       알고리즘       | m이 클 때 | 외부 메모리 학습 지원 | n이 클 때 | 스케일 조정 필요 |
| :------------------: | :-------: | :-------------------: | :-------: | :--------------: |
|      정규방정식      |   빠름    |          No           |   느림    |        No        |
|   배치 경사 하강법   |   느림    |          No           |   빠름    |       Yes        |
|  확률적 경사 하강법  |   빠름    |          Yes          |   빠름    |       Yes        |
| 미니배치 경사 하강법 |   빠름    |          Yes          |   빠름    |       Yes        |



## 다항 회귀

비선형 데이터를 학습하는 데 선형 모델을 사용할 수 있습니다. 이렇게 하는 간단한 방법은 각 특성의 거듭제곱을 새로운 특성으로 추가하고, 이 확장된 특성을 포함한 데이터셋에 선형 모델을 훈련시키는 것입니다. 이런 기법을 다항 회귀라고 합니다.



### 학습 곡선

모델이 과대적합 또는 과소적합되었는지 알 수 있는 방법 중 하나는 학습 곡선을 살펴보는 것입니다. 이 그래프는 훈련 세트와 검증 세트의 모델 성능을 훈련 세트 크기의 함수로 나타냅니다. 이 그래프를 생성하기 위해서는 단순히 훈련 세트에서 크기가 다른 서브 세트를 만들어 모델을 여러 번 훈련시키면 됩니다.

- TIP : 모델이 훈련 데이터에 과소적합되어 있다면 훈련 샘플을 더 추가해도 효과가 없습니다. 더 복잡한 모델을 사용하거나 더 나은 특성을 선택해야 합니다.
- TIP : 과대적합 모델을 개선하는 한 가지 방법은 검증 오차가 훈련 오차에 근접할 때까지 더 많은 훈련 데이터를 추가하는 것입니다.



### 편향/분산 트레이드오프

- 편향 : 일반화 오차 중에서 편향은 잘못된 가정으로 인한 것입니다. 예를 들어 데이터가 실제로는 2차인데 선형으로 가정하는 경우입니다. 편향이 큰 모델은 훈련 데이터에 과소적합되기 쉽습니다.
- 분산 : 분산은 훈련 데이터에 있는 작은 변동에 모델이 과도하게 민감하기 때문에 나타납니다. 자유도가 높은 모델이 높은 분산을 가지기 쉬워 훈련 데이터에 과대적합되는 경향이 있습니다.
- 줄일 수 없는 오차 : 줄일 수 없는 오차는 데이터 자체에 있는 노이즈 때문에 발생합니다. 이 오차를 줄일 수 있는 유일한 방법은 데이터에서 노이즈를 제거하는 것입니다.



### 릿지 회귀

릿지 회귀(또는 티호노프 규제)는 규제가 추가된 선형 회귀 버전입니다. 규제항이 비용 함수에 추가됩니다. 이는 모델의 가중치가 가능한 한 작게 유지되도록 노력합니다. 규제항은 훈련하는 동안에만 비용 함수에 추가됩니다. 모델의 훈련이 끝나면 모델의 성능을 규제가 없는 성능 지표로 평가합니다.

하이퍼파라미터 α는 모델을 얼마나 많이 규제할지 조절합니다. α=0이면 릿지 회귀는 선형 회귀와 같아집니다. α가 아주 크면 모든 가중치가 거의 0에 가까워지고 결국 데이터의 평균을 지나는 수평선이 됩니다.



### 라쏘 회귀

라쏘 회귀는 선형 회귀의 또 다른 규제된 버전입니다. 릿지 회귀처럼 비용 함수에 규제항을 더하지만 L2 노름의 제곱을 2로 나눈 것 대신 가중치 벡터의 L1 노름을 사용합니다.

라쏘 회귀의 중요한 특징은 덜 중요한 특성의 가중치를 완전히 제거하려고 한다는 점입니다. 다시 말해 라쏘 회귀는 자동으로 특성 선택을 하고 희소 모델을 만듭니다.

- TIP : 라쏘 비용 함수에서 배치 경사 하강법의 경로가 종착지로 가는 통로에서 지그재그로 튀는 경향을 보입니다. 이는 θ = 0에서 갑자기 기울기가 바뀌기 때문입니다. 전역 최솟값에 수렴하기 위해서는 학습률을 점진적으로 줄여나가야 합니다.

라쏘의 비용 함수는 θ = 0에서 미분 가능하지 않습니다. 하지만 θ = 0일 때 서브그래디언트 벡터 g를 사용하면 경사 하강법을 적용하는 데 문제가 없습니다.



### 엘라스틱넷

엘라스틱넷은 릿지 회귀와 라쏘 회귀를 절충한 모델입니다. 규제항은 릿지와 회귀의 규제항을 단순히 더해서 사용하며, 혼합 정도는 혼합 비율 r을 사용해 조절합니다. r=0이면 엘라스틱넷은 릿지 회귀와 같고, r=1이면 라쏘 회귀와 같습니다.



### 선형 회귀 사용

그럼 보통의 선형 회귀(규제 없음), 릿지, 라쏘, 엘라스틱넷을 언제 사용해야 할까요? 적어도 규제가 약간 있는 것이 대부분의 경우에 좋으므로 일반적으로 평범한 선형 회귀는 피해야 합니다. 릿지가 기본이 되지만 실제로 쓰이는 특성이 몇 개 뿐이라고 의심되면 라쏘나 엘라스틱넷이 더 낫습니다. 특성 수가 훈련 샘플 수보다 많거나 특성 몇 개가 강하게 연관되어 있을 때는 보통 라쏘가 문제를 일으키므로 라쏘보다는 엘라스틱넷을 선호합니다.



### 조기 종료

경사 하강법과 같은 반복적인 학습 알고리즘을 규제하는 아주 색다른 방식은 검증 에러가 최솟값에 도달하면 바로 훈련을 중지시키는 것입니다. 이를 조기 종료라고 합니다.



## 로지스틱 회귀

로지스틱 회귀는 샘플이 특정 클래스에 속할 확률을 추정하는 데 널리 사용됩니다. 추정 확률이 50%가 넘으면 모델은 그 샘플이 해당 클래스에 속한다고 예측합니다. 아니면 클래스에 속하지 않는다고 예측합니다. 이를 이진 분류기라고 합니다.



### 확률 추정

선형 회귀 모델과 같이 로지스틱 회귀 모델은 입력 특성의 가중치 합을 계산합니다. 대신 선형 회귀처럼 바로 결과를 출력하지 않고 결괏값의 로지스틱을 출력합니다. 로지스틱은 0과 1 사이의 값을 출력하는 시그모이드 함수입니다.



### 훈련과 비용 함수

전체 훈련 세트에 대한 비용 함수는 모든 훈련 샘플의 비용을 평균화한 것입니다. 이를 로그 손실이라 부릅니다. 안타깝게도 이 비용 함수의 최솟값을 계산하는 알려진 해가 없습니다. 하지만 이 비용 함수는 볼록 함수이므로 경사 하강법이 전역 최솟값을 찾는 것을 보장합니다.



### 소프트맥스 회귀

로지스틱 회귀 모델은 여러 개의 이진 분류기를 훈련시켜 연결하지 않고 직접 다중 클래스를 지원하도록 일반화될 수 있습니다. 이를 소프트맥스 회귀 또는 다항 로지스틱 회귀라고 합니다. 샘플 x가 주어지면 먼저 소프트맥스 회귀 모델이 각 클래스 k에 대한 점수를 계산하고, 그 점수에 소프트맥스 함수를 적용하여 각 클래스의 확률을 추정합니다. 로지스틱 회귀 분류기와 마찬가지로 소프트맥스 회귀 분류기는 추정 확률이 가장 높은 클래스를 선택합니다.

크로스 엔트로피 비용 함수를 최소화하는 것은 타깃 클래스에 대해 낮은 확률을 예측하는 모델을 억제하므로 이 목적에 부합합니다. 크로스 엔트로피는 추정된 클래스의 확률이 타깃 클래스에 얼마나 잘 맞는지 측정하는 용도로 종종 사용됩니다.